"""
search.py

–ú–æ–¥—É–ª—å –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ BM25, bi-encoder –∏ cross-encoder –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- sentence-transformers
- rank_bm25
- sklearn
- pandas, numpy, torch
"""
import pandas as pd
from tqdm import tqdm
import re
import torch
import numpy as np
import pandas as pd
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.metrics.pairwise import cosine_similarity


class HybridRetrievalModel:
    """
    –ö–ª–∞—Å—Å –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π BM25, bi-encoder –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) cross-encoder
    –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É.

    –ê—Ç—Ä–∏–±—É—Ç—ã:
    ----------
    top_k : int
        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è rerank-–∞.
    model : SentenceTransformer
        Bi-encoder –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    cross_encoder : CrossEncoder
        –ú–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞).
    use_cross_encoder : bool
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ cross-encoder –≤ pipeline.
    expand_chunks : bool
        –†–∞—Å—à–∏—Ä—è—Ç—å –ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π chunk —Å–æ—Å–µ–¥–Ω–∏–º–∏ (–∏–∑ chunk_df).
    chunk_df : pd.DataFrame
        DataFrame —Å chunk'–∞–º–∏ –∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ `document_id`, `chunk`.
    verbose : bool
        –ü–µ—á–∞—Ç–∞—Ç—å –ª–∏ –ª–æ–≥–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.
    """

    def __init__(
        self,
        top_k=10,
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        cross_model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
        use_cross_encoder=True,
        expand_chunks=False,
        chunk_df=None,
        verbose=False
    ):
        self.top_k = top_k
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = SentenceTransformer(model_name, device=self.device)
        self.cross_encoder = CrossEncoder(cross_model_name, device=self.device) if use_cross_encoder else None
        self.use_cross_encoder = use_cross_encoder
        self.expand_chunks = expand_chunks
        self.chunk_df = chunk_df
        self.verbose = verbose

        self.bm25 = None
        self.corpus_embeddings = None
        self.doc_ids = None
        self.train_texts = None
        self.qa_df = None

    def preprocess(self, text):
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è.

        Parameters
        ----------
        text : str
            –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.

        Returns
        -------
        List[str]
            –¢–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞.
        """
        text = str(text).lower()
        text = re.sub(r"[^\w\s]", "", text)
        return text.split()

    def prepare(self, qa_df, know_df):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BM25.

        Parameters
        ----------
        qa_df : pd.DataFrame
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ `question`, `answer`, `document_id`.
        """

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        def tokenize(text):
            return str(text).lower().split()

        # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö chunks –∏–∑ know_df
        know_df['tokens'] = know_df['chunk'].apply(tokenize)
        bm25_chunks = BM25Okapi(know_df['tokens'].tolist())

        # –ü—Ä–∏–≤—è–∑–∫–∞ question ‚Üí document_id
        def map_question_to_doc_id(question, k=1):
            tokens = tokenize(question)
            scores = bm25_chunks.get_scores(tokens)
            best_idx = np.argmax(scores)
            return know_df.iloc[best_idx]['document_id']

        # –ü—Ä–∏–º–µ–Ω–∏–º –∫ qa_pairs_df
        tqdm.pandas(desc="üîó Matching to document_id")
        qa_df['document_id'] = qa_df['question'].progress_apply(map_question_to_doc_id)

        qa_df = qa_df.dropna(subset=["question", "answer", "document_id"]).reset_index(drop=True)
        self.qa_df = qa_df.copy()

        self.train_texts = (qa_df["question"] + " " + qa_df["answer"]).astype(str).tolist()
        self.tokenized_corpus = [self.preprocess(text) for text in self.train_texts]

        self.bm25 = BM25Okapi(self.tokenized_corpus)
        self.corpus_embeddings = self.model.encode(
            self.train_texts,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        self.doc_ids = qa_df["document_id"].tolist()

        if self.verbose:
            print(f"‚úÖ –ö–æ—Ä–ø—É—Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: {len(self.train_texts)} —Ç–µ–∫—Å—Ç–æ–≤")

    def expand_chunk_by_neighbors(self, idx, window=1):
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ—Å–µ–¥–Ω–∏–º–∏ chunk'–∞–º–∏ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞).

        Parameters
        ----------
        idx : int
            –ò–Ω–¥–µ–∫—Å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ chunk'–∞.
        window : int, optional
            –ß–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π —Å–ª–µ–≤–∞ –∏ —Å–ø—Ä–∞–≤–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1).

        Returns
        -------
        str
            –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
        """
        if self.chunk_df is None:
            return self.train_texts[idx]

        doc_id = self.doc_ids[idx]
        doc_chunks = self.chunk_df[self.chunk_df["document_id"] == doc_id].reset_index(drop=True)

        try:
            center_idx = idx % len(doc_chunks)
        except ZeroDivisionError:
            return self.train_texts[idx]

        start = max(0, center_idx - window)
        end = min(len(doc_chunks), center_idx + window + 1)
        return " ".join(doc_chunks.iloc[start:end]["chunk"].tolist())

    def search(self, query, return_all=False):
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BM25 + bi-encoder + (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) cross-encoder.

        Parameters
        ----------
        query : str
            –í—Ö–æ–¥–Ω–æ–π –≤–æ–ø—Ä–æ—Å.
        return_all : bool, optional
            –í–µ—Ä–Ω—É—Ç—å –ª–∏ —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False).

        Returns
        -------
        Union[dict, pd.DataFrame]
            –ï—Å–ª–∏ return_all=False: —Å–ª–æ–≤–∞—Ä—å —Å –ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.
            –ï—Å–ª–∏ return_all=True: DataFrame —Å —Ç–æ–ø-N –∫–∞–Ω–¥–∏–¥–∞—Ç–∞–º–∏ –∏ –∏—Ö –º–µ—Ç—Ä–∏–∫–∞–º–∏.
        """
        if self.bm25 is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ –º–µ—Ç–æ–¥ `.prepare()` –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∫–æ—Ä–ø—É—Å–∞.")

        query_tokens = self.preprocess(query)
        bm25_scores = self.bm25.get_scores(query_tokens)
        top_indices = np.argsort(bm25_scores)[-self.top_k:][::-1]

        query_emb = self.model.encode(query, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)
        candidate_embs = self.corpus_embeddings[top_indices]
        sims = cosine_similarity(query_emb, candidate_embs)[0]

        candidates = [self.train_texts[i] for i in top_indices]
        doc_ids_top = [self.doc_ids[i] for i in top_indices]

        if self.use_cross_encoder:
            cross_scores = self.cross_encoder.predict([(query, cand) for cand in candidates])
            rerank_scores = cross_scores
        else:
            rerank_scores = sims

        best_idx_rel = int(np.argmax(rerank_scores))
        best_idx = top_indices[best_idx_rel]

        final_chunk = self.expand_chunk_by_neighbors(best_idx) if self.expand_chunks else self.train_texts[best_idx]

        if return_all:
            results_df = pd.DataFrame({
                "doc_id": doc_ids_top,
                "text": candidates,
                "bm25_score": [bm25_scores[i] for i in top_indices],
                "cosine_sim": sims,
                "rerank_score": rerank_scores,
            })
            return results_df.sort_values(by="rerank_score", ascending=False).reset_index(drop=True)

        return {
            "best_doc_id": self.doc_ids[best_idx],
            "best_chunk": final_chunk,
            "bm25_rank": best_idx,
            "bm25_score": bm25_scores[best_idx],
        }

    def retrieve(self, query, know_df=None):
        """
        –ú–µ—Ç–æ–¥-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (chunk) –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

        Parameters
        ----------
        query : str
            –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        know_df : pd.DataFrame, optional
            –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π DataFrame —Å chunk'–∞–º–∏. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –±–µ–∑ chunk_df.

        Returns
        -------
        List[str]
            –°–ø–∏—Å–æ–∫ —Å –æ–¥–Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º (chunk).
        """
        result = self.search(query, return_all=False)
        return [result["best_chunk"]]